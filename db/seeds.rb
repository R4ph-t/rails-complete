# Clear existing data
Comment.destroy_all
Article.destroy_all

# Realistic authors
authors = [
  "Mia Chen",
  "James Okafor",
  "Priya Sharma",
  "Luca Rossi",
  "Amara Diallo",
  "Ethan Brooks",
  "Yuki Tanaka",
  "Sofia Martinez"
]

# Realistic articles about technology, software, and cloud computing
articles_data = [
  {
    title: "Understanding Database Migrations in Production",
    body: "Database migrations are one of the most critical operations in any production system. A poorly planned migration can lead to downtime, data loss, or corrupted records. In this article, we explore best practices for running migrations safely.\n\nFirst, always back up your database before running any migration. This seems obvious, but it's surprising how often teams skip this step under time pressure. A backup gives you a safety net if something goes wrong.\n\nSecond, test your migrations against a copy of production data. Your development database might have 50 rows, but production could have millions. A migration that runs in milliseconds locally might take hours in production and lock critical tables.\n\nThird, consider using zero-downtime migration strategies. Tools like strong_migrations for Rails can help you avoid common pitfalls like adding indexes without the CONCURRENTLY flag or renaming columns that are still being referenced by running application code.\n\nFinally, always have a rollback plan. Every migration should be reversible, and you should test the rollback path just as thoroughly as the forward path."
  },
  {
    title: "The Rise of Platform Engineering Teams",
    body: "Platform engineering has emerged as one of the fastest-growing disciplines in software development. Unlike traditional DevOps roles that often blur the line between development and operations, platform engineering focuses specifically on building internal developer platforms (IDPs) that improve developer experience and productivity.\n\nThe core idea is simple: instead of every development team reinventing the wheel for deployment, monitoring, and infrastructure management, a dedicated platform team builds golden paths that make it easy to do the right thing.\n\nCompanies like Spotify, Netflix, and Airbnb pioneered this approach, but it's now spreading to organizations of all sizes. The key benefit is reduced cognitive load on application developers. They don't need to understand Kubernetes internals or cloud provider APIs; they just need to know how to use the platform.\n\nHowever, building a successful platform team requires more than just good engineering. It requires treating other developers as customers, gathering feedback, and iterating on the developer experience just as you would with an external product."
  },
  {
    title: "PostgreSQL Performance Tuning: A Practical Guide",
    body: "PostgreSQL is an incredibly powerful database, but out-of-the-box settings are intentionally conservative to run on minimal hardware. If you're running Postgres in production, you almost certainly need to tune it.\n\nThe most impactful settings to adjust are shared_buffers, effective_cache_size, and work_mem. Shared_buffers should typically be set to about 25% of your system's RAM. Effective_cache_size should be set to about 75% of total RAM, as it tells the query planner how much memory is available for caching.\n\nWork_mem is trickier because it's allocated per-operation, not per-connection. If you set it too high and have many concurrent queries doing sorts or hash joins, you can quickly exhaust your memory. Start conservative and increase based on EXPLAIN ANALYZE output.\n\nBeyond configuration, indexing strategy is crucial. Use EXPLAIN ANALYZE religiously. Look for sequential scans on large tables, and consider adding indexes for your most common query patterns. But don't over-index, as each index slows down writes and consumes disk space.\n\nPartial indexes are an often-overlooked PostgreSQL feature that can be incredibly effective. If you frequently query WHERE status = 'active', a partial index on that condition will be smaller and faster than a full index."
  },
  {
    title: "Building Resilient Background Job Systems",
    body: "Background jobs are the backbone of most web applications. From sending emails to processing images to generating reports, they handle the work that's too slow or too resource-intensive for a web request.\n\nBut building a reliable background job system requires careful thought. The most important principle is idempotency: every job should be safe to run multiple times with the same arguments. Network failures, process crashes, and retry logic all mean your job might execute more than once.\n\nDesign your jobs to check for already-completed work before proceeding. For example, if a job sends a welcome email, it should check whether the email was already sent before sending another one.\n\nAnother critical consideration is error handling. Not all errors are created equal. A timeout connecting to an external API is probably transient and worth retrying. A validation error in your data is permanent and retrying will never fix it. Your job system should distinguish between these cases.\n\nFinally, monitor your queues. A growing queue backlog is an early warning sign of problems. Set up alerts for queue depth, job processing time, and failure rates. The earlier you catch issues, the easier they are to fix."
  },
  {
    title: "Migrating from Heroku to Modern Cloud Platforms",
    body: "Heroku revolutionized application deployment when it launched, making it possible to go from code to production in minutes. But as applications grow, many teams find themselves looking at alternatives that offer more flexibility, better pricing, or additional features.\n\nThe good news is that the Heroku ecosystem has always encouraged good practices that make migration easier. If your app follows the twelve-factor methodology, reads configuration from environment variables, and uses managed add-ons for databases and caches, you're already well-positioned for a move.\n\nThe first step in any migration is understanding your current infrastructure. Document every service, add-on, environment variable, and scheduled job. Pay special attention to database connections, file storage, and any Heroku-specific features you rely on like review apps or pipelines.\n\nNext, choose your target platform carefully. Options range from other PaaS providers like Render, Railway, and Fly.io, to container orchestration with Kubernetes, to serverless architectures. Each has different trade-offs in terms of complexity, cost, and capability.\n\nThe database migration is usually the most critical and nerve-wracking part. Plan for a maintenance window, use pg_dump and pg_restore for PostgreSQL, and verify your data integrity thoroughly before switching over. Consider running both systems in parallel during a transition period."
  },
  {
    title: "Caching Strategies for Rails Applications",
    body: "Caching is one of the most effective ways to improve the performance of a Rails application. But with so many caching options available, it can be hard to know where to start.\n\nRails provides several built-in caching mechanisms. Fragment caching lets you cache individual view components, which is ideal for expensive partials that don't change frequently. Russian doll caching takes this further by nesting cached fragments, so that updating an inner fragment automatically invalidates the outer one.\n\nLow-level caching with Rails.cache is useful for caching arbitrary data like API responses or complex query results. Redis is the most popular backing store for Rails cache, offering persistence, data structures, and excellent performance.\n\nHTTP caching is often overlooked but can be incredibly effective. Setting proper Cache-Control headers, ETags, and Last-Modified dates can eliminate unnecessary requests entirely. For public content, a CDN can serve cached responses from edge locations close to your users.\n\nThe hardest part of caching isn't the implementation. It's cache invalidation. Stale data can cause bugs that are extremely hard to diagnose. Use cache keys that include timestamps or version numbers, and prefer time-based expiration over manual invalidation when possible."
  },
  {
    title: "Observability Beyond Logging: Traces and Metrics",
    body: "Logging has been the default debugging tool for decades, but modern distributed systems need more. Observability, built on the three pillars of logs, metrics, and traces, gives you a comprehensive view of your system's behavior.\n\nMetrics tell you what is happening at an aggregate level. Request rate, error rate, and latency (the RED method) are the starting point for any service. Add resource metrics like CPU, memory, and database connection pool usage, and you have a solid foundation for alerting and dashboards.\n\nTraces show you the journey of a single request through your system. In a microservices architecture, a single user action might touch dozens of services. Without distributed tracing, debugging why a particular request was slow is nearly impossible.\n\nThe key is correlation. When an alert fires because your error rate spiked, you should be able to drill down from the metric to example traces that show the error, and from there to the specific log lines that explain why it happened.\n\nOpenTelemetry has emerged as the industry standard for instrumentation. It provides vendor-neutral APIs and SDKs for generating traces, metrics, and logs. Adopting it means you can switch between observability backends without changing your application code."
  },
  {
    title: "Container Security Best Practices for Developers",
    body: "Containers have become the standard unit of deployment, but they also introduce security considerations that many developers overlook. Security shouldn't be an afterthought; it should be built into your container workflow from the start.\n\nStart with your base image. Alpine-based images are popular because they're small, which means a smaller attack surface. But more importantly, use specific version tags rather than 'latest'. Pinning versions ensures reproducible builds and prevents unexpected changes from sneaking in.\n\nNever run your application as root inside a container. Create a dedicated user and switch to it in your Dockerfile. If your container is compromised, running as a non-root user limits the damage an attacker can do.\n\nSecrets management is another critical area. Never bake secrets into your container image. Use environment variables or, better yet, a secrets management system like HashiCorp Vault or your cloud provider's secrets manager. Secrets in environment variables can still leak through process listings or error pages, so handle them carefully.\n\nScan your images for known vulnerabilities regularly. Tools like Trivy, Snyk, and Docker Scout can identify outdated packages with known CVEs. Integrate scanning into your CI/CD pipeline so vulnerabilities are caught before they reach production.\n\nFinally, keep your images up to date. Subscribe to security advisories for your base images and key dependencies, and rebuild regularly even if your application code hasn't changed."
  },
  {
    title: "The Art of Writing Effective Database Indexes",
    body: "Database indexes are like the index in a book: they help you find what you're looking for without reading every page. But just as a book with too many indexes would be unwieldy, a database with too many indexes suffers from slow writes and wasted storage.\n\nThe most common type is a B-tree index, which is the default in PostgreSQL and most other databases. B-tree indexes work well for equality comparisons and range queries, making them suitable for most use cases.\n\nComposite indexes (indexes on multiple columns) are powerful but require understanding of column order. PostgreSQL can use a composite index on (a, b, c) for queries that filter on a, or on a and b, or on all three. But it cannot efficiently use this index for queries that only filter on b or c. Put the most selective column first.\n\nPartial indexes are a PostgreSQL-specific feature that can dramatically reduce index size. If you only need to find active users, CREATE INDEX idx_active_users ON users(email) WHERE active = true is much smaller and faster than indexing all users.\n\nExpression indexes let you index the result of a function. For case-insensitive email lookups, CREATE INDEX idx_users_email ON users(lower(email)) avoids the need to apply lower() at query time.\n\nAlways validate your indexes with EXPLAIN ANALYZE. An index that isn't being used by the query planner is just wasting space and slowing down writes. PostgreSQL's pg_stat_user_indexes view shows you which indexes are actually being used."
  },
  {
    title: "Continuous Deployment: Moving Fast Without Breaking Things",
    body: "Continuous deployment, where every commit that passes automated tests is automatically deployed to production, sounds terrifying to teams that deploy weekly or monthly. But it's actually safer than infrequent big-bang releases.\n\nThe key insight is that small changes are easier to understand, test, and roll back than large ones. If you deploy 500 lines of changes and something breaks, you have a large search space. If you deploy 20 lines and something breaks, the cause is almost certainly in those 20 lines.\n\nTo make continuous deployment work, you need a robust testing pipeline. Unit tests catch logic errors, integration tests verify that components work together, and end-to-end tests confirm that user-facing workflows function correctly. But testing alone isn't enough.\n\nFeature flags are essential for decoupling deployment from release. You can deploy new code to production without exposing it to users, then gradually roll it out while monitoring for issues. If something goes wrong, you flip the flag instead of rolling back the deployment.\n\nCanary deployments take this further by routing a small percentage of traffic to the new version. If the canary shows increased errors or latency, the rollout is automatically halted. This catches issues that testing didn't cover, like performance problems that only appear under real production load.\n\nMonitoring and alerting close the feedback loop. You need to know within minutes if a deployment caused problems, and you need the ability to roll back quickly. Automated rollback based on error rate thresholds is the gold standard."
  },
  {
    title: "Understanding Connection Pooling in Web Applications",
    body: "Database connections are expensive to establish. Each connection involves a TCP handshake, authentication, and memory allocation on both the client and server sides. Opening a new connection for every web request would be devastatingly slow.\n\nConnection pooling solves this by maintaining a pool of pre-established connections that are shared across requests. When your application needs a database connection, it borrows one from the pool. When it's done, the connection is returned to the pool rather than being closed.\n\nIn Rails, ActiveRecord has built-in connection pooling controlled by the pool setting in database.yml. The default pool size of 5 works for development but is often too small for production. Your pool size should be at least as large as your web server's thread count.\n\nFor larger deployments, an external connection pooler like PgBouncer is essential. PgBouncer sits between your application and PostgreSQL, multiplexing many application connections onto fewer database connections. This is especially important when you have multiple application servers, as each one maintains its own pool.\n\nPgBouncer offers three pooling modes: session (safest but least efficient), transaction (best balance for most applications), and statement (most efficient but limits what SQL features you can use). Transaction mode is the sweet spot for most Rails applications.\n\nWatch out for connection leaks. If your code borrows a connection and doesn't return it (due to an exception or a forgotten checkout), you'll eventually exhaust your pool and your application will hang. ActiveRecord handles this automatically for web requests, but be careful in background jobs and scripts."
  },
  {
    title: "Event-Driven Architecture: When and How to Use It",
    body: "Event-driven architecture (EDA) has gained enormous popularity in recent years, and for good reason. By decoupling producers and consumers of information, EDA enables systems that are more scalable, more resilient, and easier to extend.\n\nThe basic idea is simple: instead of component A directly calling component B, component A publishes an event describing what happened, and component B subscribes to events it cares about. This means A doesn't need to know about B, and new consumers can be added without modifying the producer.\n\nIn practice, this is implemented using a message broker like Kafka, RabbitMQ, or Redis Streams. The broker stores events and delivers them to subscribers, handling concerns like ordering, delivery guarantees, and replay.\n\nBut EDA isn't free. It introduces eventual consistency, which means different parts of your system may have different views of the current state at any given moment. This requires careful thought about how to handle queries that span multiple services.\n\nDebugging is also harder in event-driven systems. A request might trigger a chain of events across multiple services, and understanding the full flow requires distributed tracing and good logging.\n\nStart small. Don't try to convert your entire monolith to event-driven overnight. Identify specific use cases where the benefits of decoupling outweigh the complexity costs. Notification systems, audit logging, and analytics are good starting points because they're naturally asynchronous and don't need immediate consistency."
  },
  {
    title: "Ruby on Rails in 2026: Still Going Strong",
    body: "Ruby on Rails has been declared dead more times than any other framework, yet it continues to power some of the most successful applications on the web. Shopify, GitHub, Basecamp, and thousands of startups rely on Rails for their core business logic.\n\nRails 8 brought significant improvements. Solid Queue, Solid Cache, and Solid Cable replaced the need for Redis in many applications, simplifying infrastructure. The introduction of Thruster as the default production web proxy eliminated the need for Nginx in simple deployments.\n\nThe framework's philosophy of convention over configuration continues to provide enormous productivity benefits. A skilled Rails developer can build and ship features faster than in almost any other web framework, because Rails makes thousands of small decisions for you.\n\nCritics argue that Rails doesn't scale. But scaling is rarely about the framework. It's about your architecture, your database queries, your caching strategy, and your ability to identify and fix bottlenecks. The teams that struggle with scale in Rails would struggle with scale in any framework.\n\nThe Ruby ecosystem is also healthier than ever. Ruby 3.3 brought YJIT improvements that put Ruby's performance in the same ballpark as Python and PHP. The gem ecosystem is mature and well-maintained, with gems like Devise, Pundit, and Sidekiq providing battle-tested solutions for common needs.\n\nIf you're starting a new web application in 2026, Rails deserves serious consideration. Not because it's the trendiest choice, but because it's a proven, productive, and well-supported framework that lets you focus on your business logic rather than your infrastructure."
  },
  {
    title: "Zero-Downtime Deployments: A Step-by-Step Guide",
    body: "Users expect applications to be available 24/7. Planned maintenance windows are increasingly unacceptable, especially for global applications serving users across time zones. Zero-downtime deployment techniques let you ship new code without any interruption to service.\n\nThe foundation is having multiple instances of your application running behind a load balancer. During a deployment, you update instances one at a time (rolling deployment), ensuring that there are always healthy instances available to serve requests.\n\nBut rolling deployments alone aren't enough. Your application code needs to be backward compatible with the previous version, because during a deployment both versions are running simultaneously. This means database migrations need special care.\n\nThe golden rule for zero-downtime migrations: never remove something that running code depends on. Instead of renaming a column, add a new column, backfill it, update the code to use the new column, deploy, and then remove the old column in a subsequent migration.\n\nFor Rails applications, the strong_migrations gem enforces these practices automatically. It will prevent you from running migrations that could cause downtime, like adding a column with a default value on a large table (which locks the table in older PostgreSQL versions) or removing a column that the current code still references.\n\nHealth checks are another critical component. Your load balancer needs to know when an instance is ready to receive traffic. A proper health check should verify that the application can connect to its database and any other critical dependencies, not just that the process is running.\n\nFinally, practice your deployments. If deploying is painful, you'll do it less often, which means larger changes, which means more risk. Make deployment so routine and reliable that it becomes boring."
  },
  {
    title: "API Rate Limiting: Protecting Your Services",
    body: "Every API needs rate limiting. Without it, a single misbehaving client can overwhelm your service, causing slowdowns or outages for all users. Rate limiting is a fundamental part of building robust, production-ready APIs.\n\nThe simplest approach is a fixed window rate limit: allow N requests per time period. For example, 100 requests per minute. This is easy to implement using a counter in Redis with a TTL, but it has a well-known flaw: a burst of requests at the boundary of two windows effectively doubles the allowed rate.\n\nSliding window rate limiting addresses this by smoothing out the boundary. Instead of resetting the counter at the start of each window, it uses a weighted combination of the current and previous windows. This provides more predictable rate limiting with only slightly more complexity.\n\nThe token bucket algorithm is another popular choice. It models rate limiting as a bucket that's continuously filled with tokens at a fixed rate. Each request consumes a token. If the bucket is empty, the request is rejected. This naturally allows short bursts while enforcing a long-term average rate.\n\nChoose your rate limit key carefully. By IP address is common but problematic for users behind NAT or corporate proxies. By API key is better for authenticated APIs. By user account is ideal when you have authentication. Consider using multiple layers: a generous per-IP limit to stop DDoS, and a tighter per-user limit for fair usage.\n\nAlways return clear rate limit headers (X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Reset) so clients can implement backoff strategies. Return a 429 Too Many Requests status with a Retry-After header when limits are exceeded."
  },
  {
    title: "The Twelve-Factor App in Practice",
    body: "The twelve-factor app methodology, originally written by developers at Heroku, has become the de facto standard for building cloud-native applications. But reading the twelve factors and actually implementing them are very different things.\n\nFactor one, codebase, is straightforward: one repo per app, tracked in version control. But many teams struggle with factor three, config. Storing configuration in environment variables sounds simple, but managing dozens of environment variables across multiple environments quickly becomes unwieldy.\n\nThe solution is a combination of good tooling and good practices. Use a .env file for local development (but never commit it to version control). Use your platform's configuration management for staging and production. Group related variables logically and document what each one does.\n\nFactor six, processes, requires that your application be stateless. This means no sticky sessions, no local file storage that persists across requests, and no in-memory state that isn't backed by a database. If you need to store files, use object storage like S3. If you need sessions, store them in Redis or your database.\n\nFactor eleven, logs, is more nuanced than it appears. Your application should write logs to stdout, treating them as an event stream. The execution environment is responsible for capturing, routing, and storing those logs. This separation means your application doesn't need to know about log files, log rotation, or log aggregation services.\n\nThe twelve factors aren't just academic ideals. They're practical guidelines that make your application easier to deploy, scale, and maintain. Even if you can't follow all twelve perfectly, moving in their direction will improve your operations."
  },
  {
    title: "Debugging Memory Leaks in Ruby Applications",
    body: "Memory leaks in Ruby applications are insidious. They don't crash your app immediately; instead, memory usage slowly grows over hours or days until the process is killed by the OOM killer or performance degrades to the point of unusability.\n\nThe first step in debugging a memory leak is confirming that it exists. Monitor your application's resident set size (RSS) over time. If it grows continuously without plateauing, you probably have a leak. If it grows during peak traffic and shrinks during off-hours, that's normal garbage collection behavior, not a leak.\n\nRuby's built-in ObjectSpace module is your starting point. ObjectSpace.count_objects gives you a breakdown of objects by type. Call it periodically and look for types that grow without bound. If string count is growing, you're probably creating strings that aren't being collected.\n\nThe derailed_benchmarks gem can help identify memory bloat caused by gems. Run it against your Gemfile to see how much memory each gem consumes on require. You might be surprised to find a gem you rarely use consuming significant memory.\n\nFor production debugging, the memory_profiler gem provides detailed reports of object allocations grouped by gem, file, and line. Use it with a representative workload to identify the code that allocates the most objects.\n\nCommon causes of Ruby memory leaks include: global variables or class-level arrays that accumulate data, symbol-to-proc operations that create new proc objects, string concatenation in loops (use string interpolation or StringIO instead), and caching without size limits or TTLs.\n\nSometimes the leak isn't in Ruby at all but in a native extension. Tools like Valgrind or jemallocator can help identify memory issues in C extensions. The MALLOC_ARENA_MAX environment variable can also help reduce memory fragmentation when using glibc malloc."
  },
  {
    title: "GraphQL vs REST: Making the Right Choice",
    body: "The GraphQL vs REST debate has been raging for years, and the answer remains the same: it depends. Both are excellent tools for different situations, and choosing the right one requires understanding your specific needs.\n\nREST excels when you have well-defined resources with predictable access patterns. A CRUD API for managing users, products, or articles maps naturally to REST endpoints. REST is also simpler to cache (both at the HTTP level and with CDNs), simpler to secure (each endpoint can have its own authorization rules), and simpler for new developers to understand.\n\nGraphQL shines when clients have diverse data needs. If your mobile app needs a subset of the data that your web app needs, or if different pages need different combinations of related data, GraphQL lets clients request exactly what they need in a single request. This eliminates over-fetching and under-fetching, which are common problems with REST APIs.\n\nBut GraphQL introduces complexity. You need a schema, resolvers, and type definitions. N+1 query problems are easy to create and require tools like DataLoader to solve. Rate limiting is harder because every query is a POST to the same endpoint. And caching is significantly more complex because the same endpoint can return different data depending on the query.\n\nFor most applications, especially in the early stages, REST is the pragmatic choice. It's well-understood, well-tooled, and gets you to market faster. Consider GraphQL when you have multiple clients with significantly different data needs, or when your API consumers would benefit from the self-documenting nature of a typed schema.\n\nAnd remember, they're not mutually exclusive. Many successful applications use REST for simple endpoints and GraphQL for complex data fetching, getting the best of both worlds."
  },
  {
    title: "Infrastructure as Code: Terraform, Pulumi, and Beyond",
    body: "Managing infrastructure manually through web consoles is a recipe for configuration drift, undocumented changes, and unreproducible environments. Infrastructure as Code (IaC) treats your infrastructure the same way you treat your application code: versioned, reviewed, tested, and automated.\n\nTerraform has been the dominant IaC tool for years, and for good reason. Its declarative approach, where you describe your desired state and Terraform figures out how to get there, is intuitive and powerful. The provider ecosystem is massive, covering every major cloud provider and hundreds of SaaS services.\n\nBut Terraform's custom language, HCL, has limitations. Complex logic is awkward, code reuse requires modules with their own learning curve, and testing is bolted on rather than built in. This has led to alternatives like Pulumi, which lets you define infrastructure using real programming languages like TypeScript, Python, or Go.\n\nPulumi's programming language approach means you get real loops, conditionals, functions, and type checking. You can use your existing testing frameworks, IDE support, and package managers. For developers who are comfortable with programming but find HCL frustrating, Pulumi is a revelation.\n\nCDK (both AWS CDK and cdktf) attempts to split the difference by generating Terraform configurations from programming languages. This gives you the expressiveness of a real language with the reliability of Terraform's state management and planning.\n\nRegardless of which tool you choose, the important thing is to start. Even imperfect IaC is better than no IaC. Begin with your most critical infrastructure, get it under version control, and expand from there."
  },
  {
    title: "Load Testing Your Application Before It's Too Late",
    body: "Load testing is like insurance: you don't appreciate its value until you need it, and by then it's too late. Too many teams discover their performance limits during a traffic spike, when users are affected and stress levels are high.\n\nThe goal of load testing isn't just to find your breaking point. It's to understand how your system behaves under different levels of load, identify bottlenecks before they become problems, and build confidence that your system can handle expected growth.\n\nStart by defining your expected traffic patterns. How many concurrent users do you expect? What's your peak-to-average ratio? Which endpoints are hit most frequently? Which operations are most expensive? Without this baseline, you're testing in the dark.\n\nTools like k6, Locust, and Apache JMeter can simulate realistic traffic patterns. k6 is particularly popular because tests are written in JavaScript, making them easy to version control and integrate into CI/CD pipelines.\n\nRamp up load gradually, not all at once. Start with a baseline test at your current traffic level, then increase by 25-50% increments. At each level, observe response times, error rates, CPU usage, memory usage, and database connection counts. The point where any of these metrics crosses an acceptable threshold is your current capacity.\n\nCommon bottlenecks include: database queries that perform well with small datasets but degrade with large ones, connection pool exhaustion under high concurrency, external API calls that add latency and can't be parallelized, and memory-intensive operations that cause garbage collection pauses.\n\nMake load testing a regular practice, not a one-time event. Run tests after significant changes, before major launches, and on a regular schedule. Your system's performance characteristics change over time as data grows and code evolves."
  },
  {
    title: "Securing Your Rails Application: A Comprehensive Checklist",
    body: "Security in Rails applications goes beyond what the framework provides out of the box. While Rails includes excellent protections against common attacks like CSRF, XSS, and SQL injection, a production application needs additional hardening.\n\nStart with authentication and authorization. Devise remains the most popular authentication gem, but make sure you enable all relevant modules: confirmable (email verification), lockable (account lockout after failed attempts), and timeoutable (session expiration). For authorization, Pundit provides a clean, testable policy pattern.\n\nForce HTTPS everywhere. Set config.force_ssl = true in your production configuration. This ensures all traffic is encrypted and prevents session hijacking through man-in-the-middle attacks. Make sure your cookies are marked as secure and httponly.\n\nContent Security Policy headers prevent XSS attacks by specifying which sources of content are allowed. Rails 7+ includes a CSP DSL that makes this easy to configure. Start with a restrictive policy and loosen it as needed, rather than starting permissive and trying to tighten later.\n\nKeep your dependencies updated. Use bundler-audit to check for known vulnerabilities in your gems, and brakeman to scan your code for security issues. Integrate both into your CI pipeline so you catch problems early.\n\nEnvironment variables for secrets are table stakes, but consider using Rails encrypted credentials for sensitive configuration. They can be safely committed to version control and are decrypted at runtime using a master key that's stored separately.\n\nFinally, log security-relevant events: login attempts, password changes, permission changes, and data exports. These audit logs are invaluable for incident response and compliance."
  }
]

# Commenter names for realistic comments
commenters = [
  "DevDave", "CodeCrafter42", "RubyRookie", "SysAdminSam",
  "FullStackFiona", "BackendBen", "CloudNerd", "DataDrivenDina",
  "SecOpsSteve", "MobileMarcus", "QA_Quinn", "InfraIrene",
  "APIArchitect", "ContainerCarl", "PipelinePat"
]

# Comment templates organized by tone
comments_thoughtful = [
  "This is an excellent deep dive. I especially appreciated the section about %{topic}. We ran into exactly this issue at my company last quarter.",
  "Great article! One thing I'd add is that %{topic} can also be addressed by using feature flags during the transition period.",
  "I've been working with this for about two years now and this matches my experience. The point about %{topic} is particularly important and often overlooked.",
  "Thanks for writing this up. I've bookmarked it for our next team discussion on %{topic}.",
  "Really well explained. I've struggled to articulate why %{topic} matters to non-technical stakeholders, and this article gives me a good framework.",
  "Solid advice. We implemented something similar and saw a 40%% improvement in our deployment confidence.",
  "This resonates with my experience. We learned the hard way about %{topic} when our production system went down for three hours.",
  "Bookmarking this for our next architecture review. The section on trade-offs is exactly what our team needs to read.",
]

comments_questioning = [
  "How does this approach scale when you have hundreds of services? We're hitting limits with our current setup.",
  "Interesting perspective. Have you considered how this changes when you're dealing with multi-region deployments?",
  "What's your recommendation for teams that are just getting started? The full approach seems overwhelming for a small team.",
  "Do you have any benchmarks comparing the different approaches you mentioned? Would love to see some numbers.",
  "How do you handle the testing complexity that comes with this? Our test suite is already slow.",
  "What about cost implications? In our experience, the infrastructure costs for this kind of setup can add up quickly.",
]

comments_sharing = [
  "We switched to this approach last year and haven't looked back. The initial investment was significant but it paid off within a few months.",
  "At my previous company we tried something similar but struggled with the migration. Key lesson: do it incrementally, not as a big bang.",
  "I wrote a companion tool for this that automates some of the manual steps. Happy to share if anyone's interested.",
  "We've been using a slightly different approach where we %{topic}, which has worked well for our scale.",
  "For anyone implementing this, I'd recommend starting with the monitoring piece first. You can't improve what you can't measure.",
]

all_comments = comments_thoughtful + comments_questioning + comments_sharing

topics = [
  "database migrations", "connection pooling", "caching invalidation",
  "monitoring and alerting", "deployment pipelines", "error handling",
  "load balancing", "security hardening", "performance tuning",
  "background job processing"
]

puts "Seeding database with realistic articles and comments..."

articles_data.each do |data|
  article = Article.create!(
    title: data[:title],
    body: data[:body],
    author: authors.sample
  )

  # Each article gets 3-8 comments
  rand(3..8).times do
    comment_template = all_comments.sample
    topic = topics.sample
    comment_body = comment_template.gsub("%{topic}", topic)

    article.comments.create!(
      body: comment_body
    )
  end
end

puts "Created #{Article.count} articles and #{Comment.count} comments"
puts "Authors: #{Article.pluck(:author).uniq.sort.join(', ')}"
